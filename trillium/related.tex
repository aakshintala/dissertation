% !TeX root = ../dissertation.tex

\section{Related Work}
\label{s:related}

%Support for GPU virtualization at the
%OS~\cite{kato2011timegraph,kato2012gdev,rossbach2011ptask,
%  silberstein2013gpufs,gpunet,rossbach11hotos,neon,rinnegan} and
%hypervisor~\cite{rossbach16vee, kaveri16vee} are active research
%areas.  A number of deployable VDI and graphics virtualization
%solutions~\cite{vmware-virtual-gpu, VGML} supporting graphics
%frameworks~\cite{directX,openGLspec,mark03cg}; VDI-targeted graphics
%cards~\cite{grid, firepro} simplify the design by mapping static
%partitions of the hardware to SR-IOV virtual functions.  GPGPU support
%solutions have been proposed for
%Xen~\cite{tian2014full,gupta2009gvim,suzuki2014gpuvm,shi2012vcuda,xen},
%KVM~\cite{huang2016building}, and VMware\cite{dowty2009gpu}.  API
%remoting forwards guest API calls, to a host, appliance VM, control
%domain OS~\cite{gupta2009gvim, vCUDA, gVirtuS,vmCUDA} or remote
%server~\cite{rCUDA,
%  rCUDAnew,GridCuda,VCL,giunta2010gpgpu,montella2012general}.  NVIDIA
%Docker~\cite{nvidia_container} exposes GPUs in a container, enabling
%docker images that can be deployed on any GPU-enabled infrastructure,
%but GPUs are assigned to containers and not shared.



%\subsection{Full virtualization}

%Full virtualization provides a virtual environment in which
%unmodified GPGPU programs can run on top of unmodified GPGPU frameworks and an unmodified
%guest OS. In principle, ability to run with no modification maximizes flexibility and
%leaves the hypervisor the
%most power to multiplex the device safely and fairly. Full virtualization designs
%from the literature~\cite{gVirt, GPUvm} suggest full virtualization overheads for GPUs
%staggering, primarily due to trap-based interposition of interactions through MMIO and memory-mapped command-queues,.
%which must be implemented with traps. Shadowing resources induces additional overheads:
%TLB flushes are required with every GPU page table update, and because GPU page faults
%are not forwarded to the host CPU, GPU page tables must be scanned on every TLB flush to keep
%GPU shadow page tables current.

\paragraphbe{Full virtualization.} \label{sec:gpuvm}
% Our evaluation compares \Trillium
% against GPUvm~\cite{GPUvm}, so we provide additional design background.
%GPUvm provides both full and para-virtualization support for
%CUDA on Kepler and Fermi (NVIDIA) GPUs for Xen, relying on
%Gdev~\cite{kato2012gdev}, an open-source CUDA 4.2
%implementation which isolates OS-level protection domains sharing GPU contexts,
%by partitioning a physical GPU into logical GPUs.
GPUvm multiplexes GPU hardware resources (such as memory and
channels) among Xen HVMs.
It exposes the \emph{native} GPU device model to every VM, so guests can run
unmodified GPU applications and device drivers. Because GPU drivers rely
on MMIO and shared memory queues as the core communication primitive,
all accesses to said MMIO and queues must be interposed on. The resulting traps are redirected to a \emph{GPU access aggregator} in the
hypervisor. GPUvm relies on shadowing to deal with privileged state changes made by the guest to contexts, channels and GPU pages tables.% to shadow
%version presented to the hardware.
%\emph{shadow GPU
%channels}, each containing several virtual (guest) channels and
%associated \emph{shadow page tables}. Shadowed
%resources enable the abstraction of privileged access to deprivileged
%guests, and provide an isolation mechanism whereby GPUvm can validate
%the correctness of changes to these structures made by guests,
%interposing and reflecting guest interactions.
%The overheads of interposition and reflecting state are the primary
%determinants of performance for GPUvm.

\paragraphbe{GPUvm Optimizations.}
\label{sec_gpuvm_opt}
GPUvm elides interposition on a memory aperture (BAR3) that is not used for
privilege-sensitive state or operations, and a lazy shadowing optimization
batches page table updates by waiting to reflect them until they are known to
be needed by GPU. A para-virtual optimization batches page table updates with hypercalls, further reducing trap overheads.


% GPUvm synchronizes guest GPU channel descriptors and shadow GPU
% descriptors by intercepting all data accesses through PCIe BARs, trapping to
% the hypervisor every time the BARs are accessed.  GPUvm
% synchronizes the contents of guest page tables and shadow page tables
% on every TLB flush and cannot intercept GPU-side page
% faults, forcing scans of the entire guest page table to find
% modified entries. A BAR3 remapping
% optimization passes-through BAR3 memory accesses, as those accesses
% do not modify privileged state devices state.
% The lazy shadowing
% optimization reduces effective shadow page table costs, performing them
% when the updated entry will be referenced which is only when memory apertures
% are read/written or at GPU kernels launch.

% GPUvm supports a number of scheduling policies including a simple FIFO
% implementation, the Xen default CREDIT~\cite{barham2003xen}, and the
% BAND (bandwidth-aware non-preemptive device) scheduler from
% Gdev~\cite{kato2012gdev}.
% \cjr{WE should probably ditch this since we don't do anything with scheduling}

\paragraphbe{API remoting.} \label{sec:api-remote}
GViM~\cite{gupta2009gvim,vCUDA,gVirtuS} and vmCUDA~\cite{vmCUDA} use
split-driver designs~\cite{pegasus,xen} to support CUDA in Xen~\cite{xen} and
ESX~\cite{esx} respectively. User-mode API remoting~\cite{rCUDA, rCUDAnew,
GridCuda,kim2012snucl,VCL, Duato:2009:EIG:1884795.1884840,
Li:2011:GRS:2066302.2066933, Xiao:2012:TAM:2310096.2310143}, does precisely
the same, without the benefits and drawbacks of hypervisor-level integration.
Conventional wisdom is that API remoting is fast because coarse APIs limit
interposition frequency, and overheads derive only from communication and
marshaling of API call data, which can often be amortized with batching:
indeed, rCUDA achieves faster-than-native performance in scenarios where API
calls are easily batched. API remoting is also conventionally thought to
sacrifice compatibility, isolation, and interposition. Coupling
virtualization with a particular API gives up compatibility by only supporting
applications written to use that API. Because an appliance VM or remote server
multiplexes calls and manages resources, isolation is also potentially lost,
and interposition is lost because the hypervisor cannot interpose on resource
management due to the coarse granularity of the interposition interface.
% While  real-time schedulers such as TimeGraph~\cite{kato2011timegraph} try to solve the problem.


\paragraphbe{Para-virtualization.}
Para-virtualization~\cite{GPUvm,dowty2009gpu,vasila-gvm16,
vmm-independent-gfx-vee07} formally means some change has been admitted to
guest artifacts (OS, user libraries or binaries) sacrificing compatibility. %, usually in service performance gains from lowering interposition.
The scope of potential guest modifications is quite wide:
GPUvm~\cite{GPUvm}, for example, supports hypercalls for page table updates~\cite{xen}.
In contrast, VMware's SVGA~\cite{dowty2009gpu} virtualization design relies
heavily on para-virtualization at several layers of the stack: the mainline
Linux source includes an SVGA-specific \texttt{vmwgfx} driver for the SVGA
virtual device, and the Mesa3D graphics stack integrates SVGA support in
user-mode~\cite{mesa}.

Para-virtual I/O~\cite{harel-efficient13atc,exitless-paravirtual-io,
vmm-bypass-atc06,self-virt-hpdc07,vmware-hosted-io-atc01,
direct-access-virt-io-atc08,self-virt-hpdc07,paradice} provides reasonable
performance while allowing the host to interpose on the guest's I/O activity,
but its performance still lags behind non-interposed I/O virtualization, SR-IOV.
ELVIS~\cite{harel-efficient13atc} unifies I/O activities and proposes running
host functionality on dedicated cores to minimize the latency and overhead.
ExitLessVIS~\cite{exitless-paravirtual-io} introduces a notification mechanism
with shared memory polling and inter-processor interrupts to improve
para-virtual I/O for I/O-bound workloads.
Other optimizations include bypassing VMM~\cite{vmm-bypass-atc06,
vmware-hosted-io-atc01,direct-access-virt-io-atc08},
offloading selected functionality onto the device~\cite{self-virt-hpdc07},
using Unix device files as the common para-virtual boundary~\cite{paradice},
etc. Rio~\cite{AmiriSani:2014:RSS:2594368.2594370}, an extension to Paradice~\cite{paradice} further shares I/O between mobile systems.

\paragraphbe{Pass-through.}
Pass-through provides a VM with full exclusive access to a physical GPU.
Virtualization hardware such as Intel VT-d~\cite{abramson2006intel} enable
pass-through independent of GPU hardware interfaces. Pass-through
provides \emph{no} multiplexing or interposition, yielding native performance
at the cost of all other essential virtualization properties.
\emph{Mediated pass-through}~\cite{gVirt}, a variant on this strategy,
takes advantage of GPU multiple context support to dedicate GPU contexts
to VMs, enabling performance-sensitive operations such as command queue
submission and DMA to interact directly with the hardware while resource
allocation is interposed using full-virtualization techniques.

\paragraphbe{SR-IOV.}
Single Root I/O Virtualization is a standard that enables a single physical
device to present itself as multiple virtual devices. A hypervisor can manage
and distribute these virtual devices to guests, effectively deferring
virtualization, scheduling, and resource management to the hardware. NVIDIA
and AMD both ship GPU cards targeted at the VDI market that use SR-IOV to
export multiple virtual GPUs from the hardware. Since hypervisor-level
management is completely bypassed, performance is great~\cite{Dong:2008:SNX:1855865.1855875}, but at the same cost
as in API-remoting.