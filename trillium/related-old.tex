% !TeX root = ../dissertation.tex

\section{Related Work}
\label{s:related}

\input{virtcomptable}


Support for GPU virtualization at the
OS~\cite{kato2011timegraph,kato2012gdev,rossbach2011ptask,
  silberstein2013gpufs,gpunet,rossbach11hotos,neon,rinnegan} and
hypervisor~\cite{rossbach16vee, kaveri16vee} are active research
areas.  A number of deployable VDI and graphics virtualization
solutions~\cite{vmware-virtual-gpu, VGML} supporting graphics
frameworks~\cite{directX,openGLspec,mark03cg}; VDI-targeted graphics
cards~\cite{grid, firepro} simplify the design by mapping static
partitions of the hardware to SR-IOV virtual functions.  GPGPU support
solutions have been proposed for
Xen~\cite{tian2014full,gupta2009gvim,suzuki2014gpuvm,shi2012vcuda,xen},
KVM~\cite{huang2016building}, and VMware\cite{dowty2009gpu}.  API
remoting forwards guest API calls, to a host, appliance VM, control
domain OS~\cite{gupta2009gvim, vCUDA, gVirtuS,vmCUDA} or remote
server~\cite{rCUDA,
  rCUDAnew,GridCuda,VCL,giunta2010gpgpu,montella2012general}.  NVIDIA
Docker~\cite{nvidia_container} exposes GPUs in a container, enabling
docker images that can be deployed on any GPU-enabled infrastructure,
but GPUs are assigned to containers and not shared.

Table~\ref{tab:virt-comp} compares a number of virtualization systems
from the research literature, grouping them according to fundamental
technique.
We evaluate the completeness of each solution along several dimensions characterizing
fidelity (ability run with unmodified guest libraries and OS), sharing (ability to safely and
fairly multiplex GPUs across guest VMs), compatibility (ability to support a GPU device abstraction
that is independent of hardware actually present on the host), ability to support VM migration,
and performance.
The table includes performance entries for each system including the geometric
mean slowdown (execution time relative to native execution) across all reported benchmarks
where papers reported sufficient details to enable us to extract this data.
We additionally include the benchmarks used, and where possible,
a report (or estimate) of the geometric mean speedup one should \emph{expect} for using GPUs over CPUs
on the given benchmark suite using hardware similar to that used in this paper~\footnote{Where possible, these
numbers are measured by running the benchmark suites on real hardware.}
The final column in Table~\ref{tab:virt-comp} is the geometric mean expected
speedup one might expect for the given benchmarks in a system whose GPUs are virtualized similarly.
The column is computed as the expected speedup from GPUs for the given suite of benchmarks divided
by the slowdown induced by virtualization. Entries where overheads eclipse GPU-based performance
gains are marked in red; performance profitable entries are blue.

\subsection{Full virtualization}

%Full virtualization provides a virtual environment in which
%unmodified GPGPU programs can run on top of unmodified GPGPU frameworks and an unmodified
%guest OS. In principle, ability to run with no modification maximizes flexibility and
%leaves the hypervisor the
%most power to multiplex the device safely and fairly. Full virtualization designs
%from the literature~\cite{gVirt, GPUvm} suggest full virtualization overheads for GPUs
%staggering, primarily due to trap-based interposition of interactions through MMIO and memory-mapped command-queues,.
%which must be implemented with traps. Shadowing resources induces additional overheads:
%TLB flushes are required with every GPU page table update, and because GPU page faults
%are not forwarded to the host CPU, GPU page tables must be scanned on every TLB flush to keep
%GPU shadow page tables current.

\subsubsection{GPUvm (example of full virtualization)} \label{sec:gpuvm}

Our evaluation compares \Trillium
against GPUvm~\cite{GPUvm}, so we provide additional design background.
GPUvm provides both full and para-virtualization support for
CUDA on Kepler and Fermi (NVIDIA) GPUs for Xen, relying on
Gdev~\cite{kato2012gdev}, an open-source CUDA 4.0
implementation which isolates OS-level protection domains sharing GPU contexts,
by partitioning a physical GPU into logical GPUs.

GPUvm
multiplexes GPU hardware resources (such as memory, PCIe BARs, and
channels)  among Xen Hardware Virtual Machines (HVMs).
It exposes the \emph{native} GPU device model (NVIDIA
Quadro~6000 NVC0 GPU) to every VM, so guests can run
unmodified GPU applications and device drivers. Because GPU drivers rely
on MMIO and shared memory queues as the core communication primitive,
almost all interposition must be implemented by marking MMIO and queues
inaccessible and using faults on those regions as an interposition point.
Interposed operations are redirected to a \emph{GPU access
	aggregator} in the hypervisor using a simple client-server communication protocol.
GPUvm relies on shadowing to deal with privilged state, interposing and reflecting changes
made by the guest to contexts, channels and GPU pages tables to shadow
version presented to the hardware.
%\emph{shadow GPU
%channels}, each containing several virtual (guest) channels and
%associated \emph{shadow page tables}. Shadowed
%resources enable the abstraction of privileged access to deprivileged
%guests, and provide an isolation mechanism whereby GPUvm can validate
%the correctness of changes to these structures made by guests,
%interposing and reflecting guest interactions.
%The overheads of interposition and reflecting state are the primary
%determinants of performance for GPUvm.


\paragraphbe{GPUvm Optimizations.}
\label{sec_gpuvm_opt}
GPUvm supports a number of optimizations, mostly aimed at reducing
overheads for interposition and reflection of priveleged state.
A BAR3 remapping optimization elides interposition on a memory aperture
that is not used for privilege-sensitive state or operations, and
a lazy shadowing optimization batches page table updates by wait to reflect them
until they are known to be needed by the GPU device. A paravirtual optimization
batches page table updates with hypercalls, further reducing trap overheads.


% GPUvm synchronizes guest GPU channel descriptors and shadow GPU
% descriptors by intercepting all data accesses through PCIe BARs, trapping to
% the hypervisor every time the BARs are accessed.  GPUvm
% synchronizes the contents of guest page tables and shadow page tables
% on every TLB flush and cannot intercept GPU-side page
% faults, forcing scans of the entire guest page table to find
% modified entries. A BAR3 remapping
% optimization passes-through BAR3 memory accesses, as those accesses
% do not modify privileged state devices state.
% The lazy shadowing
% optimization reduces effective shadow page table costs, performing them
% when the updated entry will be referenced which is only when memory apertures
% are read/written or at GPU kernels launch.

% GPUvm supports a number of scheduling policies including a simple FIFO
% implementation, the Xen default CREDIT~\cite{barham2003xen}, and the
% BAND (bandwidth-aware non-preemptive device) scheduler from
% Gdev~\cite{kato2012gdev}.
% \cjr{WE should probably ditch this since we don't do anything with scheduling}

\subsection{API remoting} \label{sec:api-remote}

%The design space is wide: traditionally, the server is a hypervisor or control domain OS, but user-mode daemons have been proposed
%as well, with corresponding trade-offs around isolation and fairness.

The convention of use the term to denote systems that interpose APIs in user mode
belies the generality of the mechanism.
GViM~\cite{gupta2009gvim,vCUDA,gVirtuS} and vmCUDA~\cite{vmCUDA} use split-driver designs~\cite{pegasus,xen}
to support CUDA in Xen~\cite{xen} and ESX~\cite{esx} respectively.
User-mode API remoting~\cite{rCUDA, rCUDAnew,GridCuda,kim2012snucl,VCL,
	Duato:2009:EIG:1884795.1884840,Li:2011:GRS:2066302.2066933, Xiao:2012:TAM:2310096.2310143}, by contrast, does precisely the same, without the benefits or drawbacks
of hypervisor-level integration. Conventional wisdom is that API remoting is fast because coarse APIs limit interposition
frequency, and overheads derive only from communication and marshalling of API call data, which can often be amortized with batching:
indeed, rCUDA achives faster-than-native in scenarios where API calls are easily batched.
API remoting is conventionally thought to sacrifice compatibility, isolation, and interposition. Coupling
virtualization with a particular API gives up compatibility by only supporting applications written to use that API.
Because an appliance VM or remote server multiplexes calls and manages resources, isolation is potentially lost,
and interposition is lost because the hypervisor cannot interpose on resource management, and due to the coarse granularity of
the interposition interface.

\subsection{Pass-through}

Pass-through provides a VM with full exclusive access to a physical GPU.
Hardware virtualization features such as VT-d~\cite{abramson2006intel} enable passthrough
to function without a dependence on GPU hardware interfaces. Pass-through
provides \emph{no} multiplexing or interposition, yielding native performance
at the cost of almost all essential virtualization properties.
\emph{Mediated passthrough}~\cite{gVirt}, a variant on this strategy,
takes advantage of GPU multiple context support to dedicate GPU contexts
to VMs, enabling performance-sensitive operations such as command queue
submission and DMA through to interact directly with the hardware
while resource allocation is interposed using full-virtualization techniques.


\subsection{Para-virtualization}

Para-virtualization, formally, means some change has been admitted to guest
artifacts (OS, user libraries or binaries) sacrificing
compatibility, usually in service performance gains from lowering interposition.
The scope of potential guest modifications is quite wide:
GPUvm~\cite{GPUvm}, for example, supports hypercalls for page table updates~\cite{xen}.
In contrast, VMware's SVGA~\cite{dowty2009gpu} virtualization design relies heavily
on para-virtualization at several layers of the stack: the mainline
Linux source includes an SVGA-specific \texttt{vmwgfx} driver for the SVGA
virtual device, and the Mesa graphics stack integrates SVGA support
in user-mode~\cite{mesa}.

\subsection{SR-IOV}

Single Root I/O virtualization is a standard enabling a single device
to present multiple virtual devices, but which appear to the OS as separate
physical devices. A hypervisor can manage and distribute these virtual
devices to guests, effectively deferring virtualization, scheduling,
and resource management to the hardware. NVIDIA and AMD both ship
GPU cards targeted at the VDI market that use SR-IOV to export multiple
virtual GPUs from the hardware. Since hypervisor-level management is
completely bypassed, this technique has clear performance implications,
but completely bypasses the hypervisor.

\hyu{Address review A: In spite of extensive discussion of related work, it is
	still incomplete.  For example, it is missing the MobiSys 2014 work on
	Rio by Ardalan Amiri Sani et al.  Also, the ASPLOS 2014 work by many
	of the same authors on I/O paravirtualization at the device file
	boundary.  Both of these are quite relevant to GPU virtualization.}

\subsection{Things still to cite}

\begin{compactitem}
\item SR-IOV Networking in Xen~\cite{Dong:2008:SNX:1855865.1855875}.
\item Towards exitless and efficient paravirtual I/O~\cite{exitless-paravirtual-io}.
\item TimeGraph~\cite{kato2011timegraph}.
\item Efficient and scalable paravirtual I/O system~\cite{harel-efficient13atc}.
\item VMM-independent graphics acceleration~\cite{vmm-independent-gfx-vee07}.
\item High performance VMM-bypass I/O in virtual machines~\cite{vmm-bypass-atc06}.
\item High performance and scalable I/O virtualization via self-virtualized devices~\cite{self-virt-hpdc07}.
\item Protection strategies for direct access to virtualized I/O devices~\cite{direct-access-virt-io-atc08}.
\item Virtualizing I/O Devices on VMware Workstation's Hosted Virtual Machine Monitor~\cite{vmware-hosted-io-atc01}.
\item Rio: a system solution for sharing i/o between mobile systems~\cite{AmiriSani:2014:RSS:2594368.2594370}.
\end{compactitem}
