% !TeX root = ../dissertation.tex

\section{Results from Published Systems}
\label{sec:pub-results}


Table~\ref{tab:virt-comp} shows a comparison of virtualization systems in the research literature,
classifying them (sometimes tenuously) according the technique taxonomy described in previous sections.
We evaluate the completeness of each solution along several dimensions:
fidelity (ability run with unmodified guest libraries and OS), sharing (ability to safely and
fairly multiplex GPUs across guest VMs), compatibility (ability to support a GPU device abstraction
that is independent of hardware actually present on the host), ability to support VM mobility,
and performance.

\subsection{Comparison methodology}

Our goal in reducing the performance of a research system to an aggregate number or two,
is to arrive at ``back-of-the-envelop'' estimates that can ultimate inform some intuition of
the impact of fundamental design tradeoffs on {\it expected} performance. The table includes
two performance entries for each system include the geometric
mean relative execution time (relative to native execution) across all reported benchmarks. The {\emph base} entry
corresponds to the reported numbers for the base implementation of the system; since many systems
report performance for a number of optimizations (some more realistic than others), we
additionally include an {\emph opt} entry which, if populated, is the geometric mean relative execution time for the
most highly optimized variant described. Intuitively, a realistic estimate of expected overheads
for a similar design most likely fall somewhere between the {\emph base} and {\emph opt} numbers.
Under performance, we additionally include the benchmarks used, and where possible,
a report (or estimate) of the geometric mean speedup one should expect for using GPUs over CPUs
on the given benchmark suite. It should go without saying, that if the overheads induced by
virtualization overwhelm the expected speedup, the case for using GPUs in that context
is significantly weakened. The final column in Table~\ref{tab:virt-comp} is the geometric mean expected
speedup one might expect for the given benchmarks in a system whose GPUs are virtualized similarly.
The column is computed as the expected speedup from GPUs for the given suite of benchmarks divided
by the slowdown induced by virtualization. Entries where overheads eclipse GPU-based performance
gains are marked in red; performance profitable entries are blue.

An ideal system would leave guest libraries and OS unmodified, provide strong isolation
with fairness guarantees, maintain compatibility across diverse GPU hardware, and preserve
GPU performance profitability. More succinctly, an ideal system would have all plusses
in the qualitative columns, with an {\bf expect-spdup} column that differs negligbly from
the {\bf gpu-spdup} column. An acceptable system might relax this along any axis, with
the caveat that {\bf expect-spdup} must be blue, indicating the preservation of at least
{\it some} of the expected performance profitability of GPU execution.

\subsection{Dominant Trends}

Two trends are clearly expressed in this table. First, {\bf no proposals address compatibility},
suggesting it is at once a serious challenge and an ideal research opportunity. Second,
The performance profitability of GPU acceleration is at risk if full virtualization overheads
reported in the literature are to be believed.

\subsection{Additional Considerations}

GPUvm~\cite{GPUvm} warrants entries under multiple virtualization techniques in Table~\ref{tab:virt-comp}
because the authors (to their credit) built a large number of variants of their system to characterize
the performance impact of a large range of fundamental design tradeoffs. Specifically, while full virtualization
is the stated goal of the effort, the authors implemented a number of variants using para-virtualization techniques,
along with a simple pass-through variant. Indeed, the characterization of the space is sufficiently prolific to
challenge summarization. The {\emph base} and {\emph opt} variations shown in Table~\ref{tab:virt-comp}
for full virtualization represent, respectively, a straightforward implementation which shadows GPU
resources (contexts, channels, page tables), and the most optimized variant presented in the paper,
which combines lazy shadowing with a BAR remapping which effectively passes through BAR accesses other
than those made to GPU channel descriptors to achieve significant speedups relative to the basic implementation.
The {\emph base} variation shown for GPUvm under para-virtualization require the guest to issue hypercalls
to make GPU page table updates, similar to direct-paging in Xen~\cite{xen}; the {\emph opt} variant
builds on this, providing a multicall interface to batch those hypercalls, again borrowed from Xen.
We include a GPUvm entry in Table~\ref{tab:virt-comp} for pass-through as well. While there are no technical
insights to be gained the empirical results are interesting. In keeping with intuition, overheads are neglible.
In contrast to intuition, pass through actually outperforms native in nearly 50\% of the Rodinia benchmarks.

In many cases, reporting gpu:cpu performance ratios and expected performance after virtualization
overheads are accounted is either inappropriate or impossible. For example, gVirt is included
in this table because it claims full virtualization, we cannot report expected
speedups because the evaluation relies entirely on graphics workloads, for which we have no way to
obtain reasonable sequential baselines. The same is true for SVGA2.

Several systems are evaluated on samples from various versions of the CUDA SDK.
GViM, vmCUDA, vCUDA, and gVirtus were evaluated on hand-picked samples from the CUDA 1.1, 5.0, 4.0 and 4.0
respectively. Since the papers do not report CPU-GPU speedup baselines, we estimate the
the expected speedup from GPUs by measuring GPU speedup on corresponding current CUDA 7.0 SDK
samples on a machine with a Tesla k20 and Intel i7 CPU. Clearly, the hardware differences from
the original papers are significant, and we encourage the reader to interpret the numbers with
that mind. Generally speaking the evaluations in these papers
are sufficiently specious that additional uncertainty induced by using
incomparable hardware is likely neglible, as we are primarily concerned with estimating a
single bit of information: will performance profitability be preserved.

We do not estimate expected speedup for the systems whose main goal is to expose GPU fabric in a
cluster setting: rCUDA, GridCUDA, SnuCL, and VCL, as evaluations of theses systems include
network overheads that would not be present in our target setting. Moreover, since these are
ultimately API remoting solutions, we consider the performance case to be sufficiently well
made by the four systems targeting single machine settings.

LoGV is present under para-virtualized systems because it leverages GPU protection hardware in the
hypervisor to enforce cross-VM isolation, with the side-effect that guest drivers must be modified
to implement that displaced functionality. We report no expected speedup first because the four micro-benchmarks
used in the evaluation are non-standard, and second because the evaluation prototype used unmodified
guest drivers, which means guests could not enforce isolation. Consequently, the numbers reported neglect
the (likely high) performance impact of a critical feature.
