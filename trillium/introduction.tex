% !TeX root = ../dissertation.tex

Compute density and programmability~\cite{nvidia_cuda, stone2010opencl,
gregory2014c++} have made GPUs the clear choice for efficiency and
performance:
Popular machine learning frameworks such as Caffe~\cite{jia2014caffe},
Tensorflow~\cite{abadi2016tensorflow}, Microsoft CNTK~\cite{yu2014cntk},
and Torch7~\cite{collobert2011torch7} rely on GPU acceleration heavily.
GPUs have made significant inroads in HPC as well: five of the top seven
supercomputers in the world are powered by GPUs~\cite{top500-Nov2018}.

Despite much prior research~\cite{rossbach16vee, kaveri16vee,
cc-numa-gpu-hpca15, abhishek-ispass16} on GPGPU virtualization, practical
options currently available to providers of virtual infrastructure all involve
bypassing the hypervisor. The most commonly adopted technique is to dedicate
GPUs to single VM instances via PCIe pass-through~\cite{AWS-gpu,gVirt},
thereby giving up the consolidation and fault tolerance benefits of
virtualization. More recently, industry players such as VMware, Dell and
BitFusion have introduced user-space API-remoting~\cite{bitfusion-whitepaper,
kim2012snucl, rCUDAnew, vmCUDA,rCUDA} based solutions as an alternative to
pass-through. API-remoting recovers the consolidation and encapsulation
benefits of virtualization but bypasses hypervisor interposition. The absence
of hypervisor interposition results in multiple disjoint resource managers
(the remote user-space API executor and the hypervisor) with no insight into
each others' decisions, thereby leading to poor decision making, and
priority-inversion problems~\cite{rossbach2011ptask}.

In order to understand why a viable virtualization scheme for GPGPUs hasn't
emerged, we set out to empirically analyze representatives of each
virtualization scheme adopted from prior work on a single platform. We chose
the Xen Virtual Machine Monitor as our empirical platform as that was the only
platform that GPUvm~\cite{suzuki2014gpuvm} ran on. GPUvm was selected as a
representative of of both full-virtual and para-virtual schemes. We built a
custom user-space API-remoting scheme that traps and forwards OpenCL API calls
using gRPC and protobuf over a local socket. This API remoting system was then
used to forward OpenCL calls to the native GPU stack provided by Nvidia, and
to an OpenCL implementation provided by Intel for their CPUs. Finally, we
retrofitted support for OpenCL to an implementation of the SVGA~\cite{
dowty2009gpu} design in Xen. We were specifically interested in SVGA as it
realizes a hybrid virtualization scheme: SVGA effectively remotes API calls
through a hypervisor controlled channel. SVGA multiplexes multiple rendering
frameworks through a hypercall interface that corresponds to the DirectX API,
and then translates these DirectX APIs back to whatever framework is available
in the host.
Our XenSVGA implementation relied on the open source Mesa GPU library, and the
Nouveau GPU driver on the GNU/Linux platform. These are the same components
used by VMware's implementation of SVGA. Enabling support for OpenCL in Mesa
and Nouveau involved implementing a compiler for SVGA's TGSI virtual ISA.
These empirical measurements are presented in section \S~\ref{sec:trilliumeval}
of this chapter.

Implementing the TGSI compiler for XenSVGA led to questions about the benefits
of having presence of a virtual ISA (vISA) for DSAs: GPUs (and most DSAs)
already support vendor-specific virtual ISAs (vISAs). A vISA like TGSI
introduces several additional steps during virtualization: the program to be
accelerated must first be translated to the hypervisor-supported vISA, and
subsequently re-translated to the ISA of the hardware vendor's vISA (before
invoking the vendor's software runtime to yet again translate it to the ISA of
the hardware). Further, the compilers used to translate to and from this vISA
must be competitive with those from the hardware vendor, and we risk obscuring
important semantic information in the original program from the hardware
vendor provided compiler. The guest compiler cannot target the native GPU
architecture, and valuable semantic information is lost to the host compiler.
Further, while incorporating a vISA compiler is possible in open frameworks
like OpenCL, the task is significantly more daunting for closed frameworks
like CUDA. Attempts to translate between TGSI and NVIDIA SASS in the
reverse-engineered GNU/Linux Nouveau driver understandably results in code
that is significantly less performant than that produced by the proprietary
stack.

Flexible interposition and strong isolation mechanisms are critical for device
management: a virtualization layer's primary goal is to enable isolated
sharing across VMs. However, a vISA in the case of DSAs serves only
compatibility, and often does so redundantly as DSA frameworks typically
subsume compilers into the device driver.
In order to test this hypothesis, we built a variant of XenSVGA, \Trillium.
\Trillium take a more flexible approach to ISA virtualization: eliding it
entirely when the host GPU stack bundles a compiler (most do), and using LLVM
IR, when necessary, to provide a common target for GPGPU drivers. \Trillium
relegates the translation from GPU source code to physical GPU ISA to the
host-resident driver. This vastly reduces complexity, eliminates a redundant
translation layer, and ensures that the GPU compiler has a high-fidelity view
of the target hardware, restoring optimization opportunities sacrificed by a
design that relies on multiple translations.

We found that the additional vISA provides little benefit; in fact, it harms
performance by necessitating a translation layer that obscures the program's
semantic information from the final vendor-provided compiler. \Trillium
outperforms GPUvm (a full virtualization system) by up to 14$\times$
(5.5$\times$ on average) and outperforms XenSVGA by as much as 7.3$\times$
(5.4$\times$ on average).

While \Trillium ultimately fails to compete with the low overheads available
from user-space API-remoting, it serves as existence proof of a viable
alternative design that preserves desirable virtualization properties such as
consolidation, hypervisor interposition, isolation, and encapsulation, without
% requiring full hardware virtualization. Observing the existence of this
% unexplored point in the design space, led us to design the virtualization
% scheme that is detailed in the later chapters of this dissertation,
% Hypervisor-Interposed Remote Acceleration (HIRA).