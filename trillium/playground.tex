% !TeX root = ../dissertation.tex
\section{Playground}
\label{sec_playground}

\subsection{Performance comparisons}

\paragraph{Metrics} Init, HtoD (MemAlloc+HtoD), Kernel, DtoH, Close. We
combine MemAlloc and HtoD time because some benchmarks use asynchronous
copy.

\paragraph{OpenCL vs CUDA} We implemented the same algorithms using
OpenCL and CUDA APIs, and compared their performance. Our experiments
show that OpenCL and CUDA runtime have similar kernel execution and
memory copy time, but OpenCL needs longer initialization time mainly for
kernel JIT compilation. Gdev has internal kernel scheduling and leads to
20x slow down.

\begin{verbatim}
gaussian 1024

gdev
Init: 25.707001
MemAlloc: 0.120000
HtoD: 9.001000
Exec: 8300.752930
DtoH: 12.928000
Close: 2.024000
Total: 8350.535156

ocl
Init: 483.151001
MemAlloc: 0.012000
HtoD: 1.602272
Exec: 421.954773
DtoH: 1.446976
Close: 0.257000
Total: 1008.859985

cuda
Exec: 425.878
Total: 517.747
\end{verbatim}

\paragraph{Full virtualization} Report relative runtime compared with
native. At least we have GPUvm running CUDA benchmarks, and we can use
the relative time to represent the overhead of full virtualization. The
data are put in \texttt{gpuvm\_cuda\_small.txt} and
\texttt{gpuvm\_cuda\_native\_small.txt}.

\paragraph{Native performance} Run OpenCL benmarks on native GPU (Quadro
6000 and Pascal 6000) as well as Intel CPU.
