% !TeX root = proposal.tex
\section{IEMTS --- A new accelerator virtualization taxonomy}
\label{sec:iemts}

Traditionally, virtualization designs have been taxonomized
according to the core techniques employed (e.g. emulation, full- or para-virtualization,
API remoting, etc.), and evaluated
in a property trade-off space comprising performance,
compatibility, interposition, and isolation. \emph{Isolation} ensures that mutually distrustful
guests cannot access each other's data or harm each other's performance. \emph{Compatibility},
characterizes how well a design preserves the freedom of
hardware and software components to evolve independently: e.g. changes in the hypervisor
should not force changes to guest software.
Virtualization provides an indirection layer between
logical and physical resources by \emph{interposing} a well-defined
interface. The quality of
interposition determines the nature of benefits (e.g. extent of consolidation) afforded by a
virtualized system~\cite{waldspurger12cacm}.

Virtualization techniques are well explored, yielding conventional
wisdom about their fundamental trade-offs. For example,
\emph{full virtualization} interposes the software-hardware interface
to provide a virtual view of the
underlying hardware. This enables guests to run unmodified OS and
application binaries, yielding high compatibility. However, hardware interfaces
for GPUs rely heavily on MMIO and communication through memory, which
necessitates page-fault-based interposition~\cite{tian2014full,
intel_kvmgt,kindratenko2009gpu,montella2012general} techniques that cripple
performance.
\emph{Para-virtual} designs export an abstract device to the guest,
but require hypervisor-specific drivers and runtime libraries in the
guest, trading compatibility for improved performance.
\emph{API remoting (or forwarding)}~\cite{gupta2009gvim, dowty2009gpu,
giunta2010gpgpu, shi2012vcuda} aggregates high-level API calls
issued in VMs, running them on the host or in a dedicated appliance VM.
This technique can provide near native performance because API calls
are infrequent, but has poor compatibility because it requires
changes in guest applications or libraries.

We argue that the current \emph{de facto} taxonomy and property trade-off space
are illustrative but not informative for GPUs:
there is a large body of research that has had little influence on practice.
First, Classifying virtualization designs as API-remoting vs. full vs.
para-virtual captures important concepts, and emergent properties compactly,
but doesn't explain their correlation to properties like performance.
Second, virtualization properties such as compatibility, isolation, and
interposition have highly context-dependent meaning and their relative value
to system designers can be hard to quantify.
Consider compatibility: there are many dimensions to compatibility (library,
hardware, OS, etc.), and each of those are commonly achieved by separate
technical, and non-technical means (e.g., TGSI is the common vISA for both the
VMware and GNU/Linux graphics stacks; this is \emph{not a lucky coincidence}).

We argue that practical design goals, such as providing a virtualization layer
with specific characteristics, get obscured when these properties are
considered as a set of constraints that must be preserved, without first
refining for context.
Further, production systems, such as VMware SVGA~\cite{dowty2009gpu},
compose multiple virtualization techniques in order to leverage the best
properties of each technique, especially in the presence of multiple
interfaces.

To enable a cleaner separation of concerns, we draw on the observation that
\textit{all} virtualization relies on encapsulation and interposition, and
note that a design can be clearly understood by identifying:
\begin{itemize}[nosep,leftmargin=1em,labelwidth=*,align=left]
\item the \textbf{I}nterface that is interposed,
\item the \textbf{E}nd-points (source and destination) the interposed event is transported between,
\item the \textbf{M}echanism used to interpose,
\item the \textbf{T}ransport mechanism used to communicate between endpoints,
\item the mechanisms used to \textbf{S}ynthesize or implement the desired functionality at the destination. We call this the \textbf{\texttt{IEMTS}} framework.
\end{itemize}

\begin{table*}[tt!]
\centering
\footnotesize
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}p{0.25\linewidth}|p{0.18\linewidth}|p{0.18\linewidth}|p{0.2\linewidth}|p{0.2\linewidth}@{}}
\toprule
\multirow{2}{*}{}         & \multirow{2}{*}{GPUvm} & \multicolumn{2}{c|}{VMware SVGA}             & \multirow{2}{*}{rCUDA} \\
                          &                        & Control Interface & GPU ABI             &                        \\ \midrule
Interposed Interface      & MMIO/BAR               & DirectX APIs      & Device ISA              & Userspace API          \\
Interposition Source      & Trap handler           & Guest driver/libs      & Guest Driver            & Guest Library           \\
Interposition Destination & Host driver            & Host framework        & Host Driver             & Host/Server Daemon     \\
Interposition Mechanism   & Trap                   & Guest library          & Compilation to vISA                    & Guest Library Shim     \\
Transport                 & Fault                  & Hypervisor FIFOs   & Hypervisor FIFOs    & RPC                    \\
Synthesis                 & Emulation              & Call host API     & Binary translation   & Call Server API        \\ \bottomrule
\end{tabular}
}
\caption{Comparing virtualization designs using the \texttt{IEMTS} framework.}
\label{tab:new-dims}
\end{table*}

\aak{Add discussion of the Table.}
