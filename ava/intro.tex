% -*- fill-column: 85; -*-
%!TEX root = ../dissertation.tex

Practical DSA virtualization must support sharing and isolation under flexible
policy with minimal overhead. The structure of current accelerator stacks
makes this combination extremely difficult to achieve.
Accelerator stacks are \emph{silos} (see \S~\ref{section:silos}) comprising
proprietary layers communicating through memory mapped interfaces.
This opaque organization makes it \emph{impractical} to interpose intermediate
layers to form an efficient and compatible virtualization boundary
(\S\ref{s:properties}). The remaining interposable interfaces leave designers
with untenable alternatives that sacrifice critical virtualization properties
such as interposition and compatibility, as we saw in prior chapters.

This chapter explores a novel hypervisor virtualization design, \hirafull
(\hira), which we argued represents a ``sweet-spot'' in the DSA virtualization
design space in the previous chapter (\S~\ref{chapter:iemts}). We explore how
the silo-ization of DSA software and hardware stacks leaves only one viable
software interface for interposition: the user-space API (this dissertation is
focused on software virtualization techniques for DSAs, and doesn't explore
hardware support for virtualization). We describe how \hira enables us to
combine the low overhead and ease-of-evolution of API-remoting with the
isolation, and effective policy enforcement of hypervisor interposition.

The API-agnostic para-virtual stack, API-specific \workers, and
hypervisor-level resource management espoused by \hira, are combined with
\lapis (a Domain-Specific Language used to capture semantic information of the
APIs interposed), and a compiler for \lapis to automate construction and
deployment of guest libraries in our prototype: \AvA. \AvA uses the abstract
para-virtual device to serve as a transport endpoint for forwarding the public
APIs of vendor-provided frameworks (e.g. CUDA or TensorFlow). Unlike currently
popular user-space API remoting solutions~\cite{bitfusion,xaas,vmCUDA,rCUDA,
cu2rcu}, \AvA preserves hypervisor-level resource management and strong
isolation by forwarding API calls over hypervisor-managed communication
channels, inserting au\-to\-ma\-tically-generated resource management
components at the transport layer to enforce policies from a DSL
specification. Critically, \emph{automation} from \AvA enables hypervisors
to keep up with fast accelerator evolution: automatic generation of components
dramatically shortens the development cycle.

\AvA supports a broad range of currently-shipping compute offload
accelerators. We virtualized \numaccelerators accelerators including NVIDIA
and AMD GPUs, Google TPUs, and Intel QuickAssist. Virtualizing an API
framework using \AvA requires modest developer effort: a single developer
virtualized OpenCL in a handful of days, a stark contrast to the person-years
of developer effort for VMware's SVGA II~\cite{dowty2009gpu} or BitFusion's
FlexDirect~\cite{bitfusion}. \AvA provides near-native performance
(e.g., 2.4\% slowdown for TensorFlow and 5.6\% for CUDA), enforces isolation
and fair sharing (\S\ref{s:properties}) across guests, and supports live
migration. \AvA is available on GitHub \mbox{\href{https://github.com/utcs-scea/ava}{utcs-scea/ava}}.

This dissertation describes the core virtualization technique, \hira, and
provides a brief overview of the design, implementation, and evaluation of
\AvA, as necessary, to discuss the efficacy of \hira as a virtualization
technique. This document elides detailed discussion of \AvA's DSL, \lapis, and
various other automation techniques used to recover compatibility in \AvA. For
detailed treatment of these topics, please refer to the published papers on
\AvA~\cite{ava-hotos,ava-asplos}, and Hangchen Yu and Arthur Peters'
dissertations.