% -*- fill-column: 85; -*-
%!TEX root = ../dissertation.tex

\subsection{Limitations}
\label{s:limitations}

%\AvA can operate in three different ways depending on the level of control it has over the virtual address spaces of the application and \worker.
%\begin{itemize}
%\item Uniform memory mode, in which buffers in the \worker's are mapped into the guest application's at the same address. With appropriate address space controls, software shared memory can be used.
%\item Shared memory mode, in which buffers are shared between the \worker and the application, but the buffer addresses may be different. Software shared memory can be used.
%\item Remote mode, in which no memory is shared and application data is only updated during API calls.
%\end{itemize}
%In uniform memory mode, \AvA requires that all pointers be passed to or returned from an API call before the buffer is used so that \AvA can setup the mapping.
%This means that pointers cannot be solely passed as part of an application-defined data structure.
%In shared memory mode, \AvA requires that pointers are \emph{always} passed via an API calls and are never passed inside application-defined data structure.
%This is required because \AvA must remap the pointers during transport to match the address space of the receiver.
%However, in shared memory mode, the application can still poll for device-side writes since the memory is shared, all be at varying addresses.
%In remote mode, ...

\AvA relies on a translation layer in which guests access API-level objects in the \worker through handles.
This introduces some limitations on support for full unified virtual memory, device-side memory allocation, and application polling for
device-side writes (discussed in \S\ref{s:api:mapped-mem}), most of which manifest only in combination with
VM migration.
\AvA is able to preserve pointer-is-a-pointer semantics for the memory translation
layer by using an identity mapping between handle-space and the \worker virtual address space.
However, \AvA's techniques
for supporting VM migration cannot be guaranteed to recreate the \worker virtual address space with exactly the same virtual mappings at the migration target,
so when state is recreated, it will be isomorphic to the source state, but that identity mapping may not be preserved. Supporting GPU mapped memory by mapping \worker memory into the
guest address space has a similar limitation: mappings cannot be reliably preserved across the migration, so \AvA cannot support migration for applications that use zero-copy.
\AvA could fix this if accelerator memory management APIs provided more control over the virtual address space.
Device-side memory allocation is not visible to \AvA through framework APIs, so migration for such workloads
is not yet supported. \AvA currently does not support demand-paging between accelerators and guest memory: this
limitation is not fundamental, and we plan to implement support for it in the near future.
\AvA does not support migration in the presence of application-level non-determinism. %other than variation buffer addresses and resource handles.\amp{There was a comment here I couldn't read.}

%Support requires shared memory between the guest application and \worker and control over virtual addresses (in the application and the \worker).
%Without these features, \AvA cannot guarantee that pointers will be valid in both contexts and will still be valid after migration.
%In some cases, shared memory can be emulated in software using existing techniques, but this may not be possible due to write originating from the device.

% After migration \emph{or} when the above features are not available, applications are only allowed to pass device memory pointers as arguments to API calls; e.i.,
% the pointers cannot be passed inside an application-defined data structure.
% This is required because in these cases, \AvA must remap pointers during transport.
% Regardless, \AvA always allows applications to perform arithmetic on device pointers, by allowing arithmetic on handles and mapping them to the appropriate pointers.
% This limitation on migration could be eliminated if the accelerator supported allocating memory at specific virtual addresses, allowing \AvA to recreate the device address space in the migration target.

% CJR:
%\begin{itemize}
%\item UVM support and pointer-is-a-pointer semantics
%\item Migration and pointer-is-a-pointer semantics
%\item Polling for device-side writes to host memory
%\item Device-side memory allocation
%\end{itemize}

%\amp{Note: A lot of these limitations parallel those of garbage collectors. What this means is that we might actually be able to avoid all of these issues with data structure and stack maps as used in GCs. This is hard to do in unmanaged languages, but it's not impossible and LLVM does have limited support for generating such maps for C/C++ programs.}
