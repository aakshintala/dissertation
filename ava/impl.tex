% -*- fill-column: 85; -*-
%!TEX root = ../dissertation.tex

\section{Implementation}
\label{s:impl}

We prototyped \AvA on Linux kernel~4.14.0 with QEMU~3.1.0 and LLVM~7.0.
Our resource management modifications to the KVM hypervisor took 1,500~LoC.
We modified the QEMU \emph{virtio-vsock} device and the corresponding
\emph{vhost-vsock} host driver to enable interposition. The para-\vdev, which
is used for both interposition and transport, was built as a QEMU display
adapter (500~LoC). The guest driver is 500~LoC long, each transport channel is
about 400~LoC on average, the \CAvA was implemented in 3,200~LoC of Python
code. Other libraries accounted for 2,000~LoC.

\subsection{Transport}
\AvA supports several interchangeable transports, allowing it to support
disaggregated hardware via \emph{sockets}, as well as local execution via
guest-\worker \emph{shared memory}. The \emph{socket} transport uses either a
TCP/IP network socket or an inter-VM socket (VSOCK) to transport commands and
data. The socket layer copies data multiple times, and incurs queuing delays.
\emph{Shared memory} provides efficient data transfer when the guest and the
\worker are on the same physical machine. The hypervisor exposes a contiguous
virtual buffer to the VM through the \vdev PCIe BAR (base address register).
The guest para-virtual driver manages the virtual BAR and assigns a partition
to each guest application. \AvA uses the shared buffer to transport buffers to
the \worker, but still uses a socket (currently VSOCK) to transport commands
to retain hypervisor interposition.

\subsection{Hypervisor Interposition and Mediation}
\AvA enables hypervisor mediation by interposing the transport channel.
We extended QEMU \emph{virtio-vsock}~\cite{virtio,virtio_vsock} (a host/guest
communication device) to build the virtual device. The corresponding
\emph{vhost-vsock} host driver was extended to perform interposition during
packet delivery.

When forwarding an API call, the command is always sent on the modified VSOCK
channel, while the argument buffer can be transferred via either VSOCK or
guest---\worker shared memory. Transferring the command via VSOCK provides a
doorbell to the router---the router then schedules the invocation based on
resource limits. The \worker and guest application have unfettered access to
the shared memory, but the \worker does not know what the requested operation
is or where the buffers are until the hypervisor forwards the command.

\subsubsection{Policies in eBPF}
\AvA supports policies written as eBPF(Extended Berkeley Packet
Filter~\cite{bpf}) programs. We defined a new eBPF program type that can be
loaded into KVM via \texttt{ioctl}. \AvA reuses the same eBPF instruction set
as socket filtering, and leverages the unmodified LLVM compiler to compile the
eBPF program. The eBPF verifier had to be modified to verify the memory
accesses of the new type of program. We provide helper functions for \AvA eBPF
programs. Leveraging eBPF allows \AvA to take advantage of eBPF program
verification at a very low cost (4.3\% of \AvA's internal overhead). The
current implementation computes resource utilization in the \worker and then
reports this utilization to the hypervisor.

\subsubsection{Scheduling}

\AvA provides a weighted fair queuing (WFQ) scheduler, with two rate control
algorithms. Each VM $v$ sharing the device is configured with one share $s_v$.
$v$'s average device time usage is $L_v=s_v/\sum_{v\in V} s_v$, where $V$ is the set of running VMs. VM~$v$'s device utilization time is accumulated into
$T_v(t)$ in the time window $[t, t+1)$. If a VM's device usage time exceeds its share $s_v$, API calls from $v$ will be postponed until its utilization proportion becomes lower than the threshold. The scheduling window is 500~ms (or the interval between two adjacent calls), and device utilization is updated upon every API completion. \AvA supports the following algorithms:

\paragraphbe{Fixed-rate polling} where the delay is a fixed interval $d$ (usually longer than the time window).

\paragraphbe{Feedback control} where the adaptive delay, $d_v(t)$, is computed by the additive-increase multiplicative-decrease (AIMD) algorithm~\cite{aimd} below ($a=1$~ms and $b=1/2$; see \S\ref{s:eval_rate_limit}).

\[
d_v\left(t+1\right)=\begin{cases}
d_v\left(t\right)\times b, &\textrm{if VM $v$ exhausted its share}\\
d_v\left(t\right)+a, &\textrm{otherwise}
\end{cases}
\]

\subsection{Shadow Resources}

\AvA supports threading and long-lived buffers by shadowing them in the
\worker. The \worker spawns a shadow thread when a new guest thread makes its
first API call, and reuses it for all future calls from that thread. For
synchronous calls, the guest thread will be blocked while the shadow thread
executes the call. Shadow threads are destroyed when the original thread is
destroyed or when the guest application exits. Similarly, the worker allocates
a new shadow buffer when it is first notified of a buffer annotated with a
long lifetime and deallocates it when the application calls a function
annotated with the corresponding annotation. Reverse shadows, guest library
buffers, and threads which shadow an \worker resource, are supported in the
same way.

\subsection{Callbacks}

When an API registers a callback, the guest library stores both the original
application \texttt{userdata/tag} value and the function pointer in a buffer.
This buffer is then supplied as the \texttt{userdata} argument to the \worker.
The \worker registers a generated stub function with the accelerator API.
When the API framework calls the stub in the \worker, a callback is made to
the guest library with the guest library buffer as the \texttt{userdata}
argument. The guest library finally extracts the original application
\texttt{userdata} value and function pointer and performs the call back into
application code. The call to the guest library uses the same protocol as
calls to the \worker, so all features of \AvA apply to callbacks. For example,
callbacks block the \worker thread that called them if the callback is
synchronous.