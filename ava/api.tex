% -*- fill-column: 85; -*-
%!TEX root = ../dissertation.tex

\begingroup % A group to prevent \txt from leaking into following files.
\newcommand{\txt}[1]{\textrm{\it #1}}
\newcommand{\halfvspace}{} % \vspace*{0.25em}
\newcommand{\halfhspace}{\hspace*{0.25em}}
\newcommand{\spec}{\lstinline[language=C,style=nwspecc,columns=fullflexible]}

\graphicspath{{images/}}

\section{\CAvA and \Lapis}
\label{s:api}
\label{s:compiler}

The \AvA toolchain
comprises a language (\Lapis), a compiler (\CAvA), and a runtime library.
% AMP: The runtime is not written in Lapis. (It will be in Lapis 2, which I may well have mentioned at some point)
\CAvA accepts code written in \Lapis (\textbf{L}ow-level \textbf{API} \textbf{S}pecification), and generates C
source for an API-specific remoting stack that implements the \hira design.
\Lapis extends C declarations with descriptors to express a broad range of semantic information
necessary to generate that stack.
This includes information captured by traditional IDLs (e.g., function parameter semantics and data layout),
as well as information required for accelerator virtualization that is not expressible in existing IDLs.
% \Lapis is not inherently specialized for accelerator API remoting--the additional semantics
% required for \AvA are expressed in terms of more general language-level primitives:
% however, to simplify discourse, we will focus only on features relevant to \AvA.

To understand how \AvA relates to other IDL-based API-remoting techniques,
it is useful to compare it to the Sun Network File System~\cite{Sandberg:1988}.
NFS supports remote access to the the file system using
an IDL to specify API semantics and a compiler to automate code generation. NFS and \AvA share a number of
key challenges. Both must marshal and transfer function calls and arguments, handle asynchrony,
refactor functionality and (potentially implicit) state across newly decoupled components. Both must
preserve the resource management and sharing support expected for a resource managed by system software.

However, key differences between \AvA and NFS arise around additional virtualization requirements and
limitations in the design space.
For example, virtualization requires that \AvA be able to capture of
sufficient API-level state to enable features like live VM migration.
Key techniques used by NFS to deal with implicit state and
resource management are impractical \AvA.
NFS mostly \emph{eliminates} implicit state by altering the API,
e.g. replacing functions using implicit seek pointers with stateless read/write functions and offset parameters.
To deal with resource management, sharing, and compatibility challenges,
NFS \emph{introduced} the VFS layer, providing
an application-transparent interposition point at which to centralize or delegate that functionality using code written
to run at that layer. \AvA cannot alter APIs, so it exposes language-level features for dealing with implicit state.
\AvA cannot introduce new abstraction layers due to vendor opacity and API diversity: so the
resource management and sharing policy are expressed at the language layer as well.

% \paragraphbe{OS interposition.} A significant challenge faced by NFS was lack of a well-defined
% OS-level API for the file system, which is necessary for preserving OS-level resource management of the file
% system and providing compatibility at a software layer that minimizes application level change. While \AvA
% has the same problem (there are no standard OS-level interfaces for accelerators), the NFS designers had
% the luxury of simply defining one: the VFS layer is introduced for precisely this purpose.

% \paragraphbe{Difficult-to-capture API semantics.} API prototypes in a native header file do not fully
% specify the semantics of that API for a number of reasons. Some of missing semantics are
% straightforward to provide with an IDL specification, e.g. \texttt{in/out} semantics for arguments are absent,
% type layout for serialization, and relationships among individual function arguments such as buffer pointers and sizes.
% Some missing semantics are more difficult to capture in an IDL, for example API functions may
% refer to or manipulate context that is carried across API calls, such as seek pointers for \texttt{read} and \texttt{write} calls,
% which presents a design challenge for how to factor such implicit state\amp{context?} across the client/server boundary.
% Unlike NFS, which had the luxury of simply changing APIs to minimize implicit state (e.g. by replacing seek position with
% explicit offset parameters, or piggy-backing generation numbers on client file handles), \AvA does not have the
% luxury of changing APIs. Moreover, \AvA must contend with a number of additional requirements not
% present in NFS: enabling \emph{hypervisor-level resource management} requires semantic knowledge the relationship between
% API calls and accelerator-level resource management (e.g. how much memory is allocated, what fraction of the compute
% cycles are dedicated to individual guests), semantic knowledge of API-level \emph{object lifetimes} to support
% features such as live migration, and the ability to express resource management \emph{policies} that can be enforced by the hypervisor.

% While extending an existing IDL would provide \AvA with necessary semantics to deal with
% basic parameter marshalling, we know of none that capture all the semantics required to enable
% state tracking for device-side objects, abilitiy to express complex implicit API state, and
% support for expressing and enforcing resource management polcies. Moreover, while the guest library
% and API servers generated by \CAvA are analogous to the stubs and skeletons created by an
% IDL compiler, \AvA requires a backend specialize to additionally generate device driver code, and
% policy hooks to run in the hypervisor. Consequently, we chose to design a new language and compiler that
% meets these needs.

% \cjr{Present CAvA and LAPIS as a development platform: runtime library support is a key element.}

\input{ava/tables/lapis-example}

\subsection{\Lapis}

A developer writing a \Lapis specification must be concerned with
capturing and expressing API semantics that fall roughly into four categories:
argument and data marshalling, asynchrony, explicit and implicit state,
and resource management policy.
To illustrate how \CAvA, \Lapis, and the runtime library,
work together to address these concerns, we will use a running example that
uses the CUDA driver API in an idiomatic scenario. The example allocates GPU memory (\texttt{cuMemAlloc}), makes
asynchronous calls using CUDA streams to transfer data before (\texttt{cuMemcpyHtoDAsync}) and after (\texttt{cuMemcpyDtoHAsync}) launching a kernel (\texttt{cuLaunch\-Kernel}),
and synchronizes the stream (\texttt{cuSynchronize\-Stream}).
The \Lapis source for the four of the five CUDA API functions required is shown in \autoref{fig:lapis-values-example}. We
elide source for \texttt{cuMemcpyHtoDAsync} because it is conceptually similar to \texttt{cuMemcpyDtoHAsync}.

\Lapis extends C types and function prototypes with information to serialize arguments and return values,
and manage user-defined metadata on API-level objects such as GPU kernels or memory buffers. The latter can be used to
specify the higher-level semantics and behaviors required for virtualization such as how to capture, transfer, and synchronize implicit state.
Descriptors are embedded in \Lapis function declarations,
can be flexibly scoped (\autoref{tab:lapis-scope}), and
applied to values (\autoref{tab:lapis-values}) or functions (\autoref{tab:lapis-functions}).
%A descriptor can also use a \Lapis keyword to apply to an \spec|argument|, an \spec|element| of a buffer, or all instances of a \spec|type|.
Descriptors can be conditional using an \spec|if| statement.
In the following discussion of \Lapis descriptors, we use the term ``this function'' to refer the function
being described or the function whose argument is being described.

\paragraphbe{Marshalling and Managing Values and Objects.}
\Lapis value descriptor usage is illustrated by the CUDA function \spec|cuMemcpyDtoHAsync| defined in \autoref{fig:lapis-values-example} (Line~\ref{line:cuMemcpyDtoHAsync}).
The argument \spec|src| is only used by the application through API calls
(\AvA does not currently support UVM, see \S\ref{s:limitations} for details).
This is true of the type \spec|CUdeviceptr| in general, so it
is expressed on line~\ref{line:cudeviceptr-handle} with \spec|type(CUdeviceptr) { handle; }|.
The \spec|handle| descriptor enables \AvA to generate code that implements an indirection layer for accessing it.
The argument \spec|dst| is a pointer to an application buffer of length \spec|size|; this is expressed on line~\ref{line:dst-nocopy} with \spec|argument(dst) { no_copy;| \spec|buffer(size); }| which selects the argument \spec|src| and specifies it as a \spec|buffer| of \spec|size| length.
The buffer will be copied back to the application later so it is \spec|no_copy| here.
Because the function is asynchronous (see below), the buffer for \spec|src| in the \worker must exist at least until the copy has completed:
the descriptor \spec|lifetime_manual| on line~\ref{line:dst-nocopy} states that the specification will indicate when the buffer is no longer needed using \spec|deallocate| (line~\ref{line:dst-buf-out}).
%On line \ref{line:src-in} \spec|in| informs \CAvA that the buffer will only be read by the call and should be copied from the application to the \worker before the call.
%The argument \spec|size| is a primitive value so \CAvA can simply copy its value directly from the application to the \worker. This is the default, so no descriptor is needed.
%The \spec|cuMemcpyDtoH| specification is similar except the types of \spec|src| and \spec|dst| are switched and the buffer argument is now annotated with \spec|out| since it will be written by the call and needs to be copied back to the application thereafter.

\input{ava/tables/lapis-scope}
\input{ava/tables/lapis-values}

\paragraphbe{Asynchrony and Implicit State.} \Lapis supports descriptors
for expressing semantics that arise due to asynchrony.
Most commonly, these are expressed by applying descriptors to functions.
For example, \spec|cuMemcpyDtoHAsync| in \autoref{fig:lapis-values-example}
is asynchronous and can return \spec|CUDA_SUCCESS| before the copy completes;
this is expressed using \spec|async| and \spec|success| on line~\ref{line:success}.
%The function descriptors in \autoref{tab:lapis-functions} specify function-level metadata,
%such as kernel parameter types or other special processing needed to make a remote call.
%The \spec|sync| and \spec|async| descriptors specify if a call from the application can return immediately while \AvA handles the call asynchronously, e.g. to improve performance.
% by overlapping \worker and application-level work.
%Asynchronous functions return a value to the application as specified by the \spec|success| descriptor (line~\ref{line:success}).
%Forwarded API calls may need to copy buffers which are not explicitly exposed as arguments to the API function.
%For example,
%because the \worker maintains shadow copies of buffers,
Asynchrony can also create implicit API-level state whose semantics must be expressed.
\spec|cuStreamSynchronize| must copy buffers back to the guest if there are outstanding asynchronous copies.
Those buffers may not be updated in the \worker until the call completes, so copy-back must be delayed to this point.
This requirement is expressed by passing dependent buffers using \spec|contextual_argument| along with the normal explicit arguments. The \Lapis code on line \ref{line:sync-bufs-1}--\ref{line:dst-buf-out}
uses this technique along with runtime library functions to track buffers dependent on ongoing asynchronous copies.
\AvA supports zero-copy transport of buffers: if a buffer allocation is described with \spec|zerocopy|, \CAvA allocates zero-copy memory for the application-side buffer.
%this is an important optimization for high-throughput APIs (e.g., kernel bypass APIs).
%The current implementation only supports this in a few specific cases, but this is not a fundamental restriction.

%\autoref{tab:lapis-functions} also includes descriptors for resource accounting and sharing policy.
%These are used in \autoref{fig:lapis-values-example}.
\paragraphbe{Resource Management and Policy.}
\Lapis supports descriptors to express the resources consumed by API functions.
Resources may be either \emph{instantaneous} or \emph{continuous}.
Instantaneous resources are consumed by an API function implementation only once,
e.g., by executing a GPU kernel upon request. Accounting works by measuring resources used at each function invocation.
In general, instantaneous resources are used to control throughput in some way; e.g.,
limiting the amount of compute resource a client is allowed to use in a fixed interval of time.
Continuous resources capture the ability an API implementation to assign a resource to a client for a
period of time. Accounting for continuous resources tracks resources assigned to each client/VM.
For example, GPU memory is a continuous resource limited by available physical memory,
which needs to be allocated
according to a sharing policy that manages cross-VM contention for it.

\autoref{fig:lapis-values-example} uses descriptors to track kernel calls (\spec|cuLaunch|\-\spec|Kernel|) and GPU memory allocation.
In \Lapis, this is expressed using two resources and descriptors on functions to specify how much of each resource is consumed.
Line~\ref{line:declare_rc} declares a resource representing function invocation rate.
Line~\ref{line:scheduler_code} specifies the custom policy used to schedule function invocations.
Line~\ref{line:consume_rc} specifies that a call to \spec|cuLaunchKernel| counts as one unit of the \spec|kernel_calls| instantaneous resource.
Line~\ref{line:allocates_rc} specifies that a call to \spec|cuMemAlloc| allocates \spec|size| bytes of the continuous resource \spec|gpu_mem|.


%For some instantaneous resources (e.g., data transfer over the bus), the amount can be estimated based on the arguments to a function call (buffer size).
%For other instantaneous resources (e.g., computation), the amount needs to be computed after the fact by measure how long the call or calls took to complete.
%Because of this, resource usage descriptors may be handled after a call completes by some Lapis compilers.

To specify policies, developers provide functions that schedule API calls from different VMs based on the recorded resource usage of those VMs. In our current implementation, policy functions are specified as eBPF programs stored in a separate file and referenced from the \Lapis source using \spec@policy("policy_kern.c")@ at the top level. In future work, we plan
to extend \Lapis to express these policies directly. We currently use eBPF because it enables unprivileged code to run
safely in the hypervisor and is available today, enabling \AvA to be used without modifying the hypervisor and without trusting the developer. However, in principle, the same properties can be achieved using \Lapis
and will provide a significant complexity reduction for the developer.
%However, the policy function could be run in privileged context if it were trusted, or could be run using any other technique for unprivileged execution inside the kernel.
%However, eBPF is already available, avoiding the requirement of additional changes to the kernel.

\input{ava/tables/lapis-functions.tex}



\begin{table}
  \centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{@{}p{1.4in}@{}p{2.7in}@{}}
  \toprule
  Metadata &  \\
  \midrule
  \spec|declare_metadata(ty)| & Specify the type of value to store as metadata to be \spec|ty|.  \\
  \spec|metadata(key)| & Get, creating if needed, the metadata object associated with \spec|key|. \\
  \midrule
  In-flight buffers &  \\
  \midrule
  \spec|buf_registry r;| & Declare \spec|r| to be a registry of buffers. \\
  \spec|register_buf(r, k,|\newline\spec| buf, size)| & registers \spec|buf| which is \spec|size| elements attached to key \spec|k| (e.g., a CUDA stream) in registry \spec|r|. \\
  \spec|get_bufs(r, k)| & gets an array of the buffers attached to key \spec|k| in registry \spec|r|. \\
  \spec|get_n_bufs(r, k)| & gets the number of buffers return from \spec|get_bufs|. \\
  \spec|get_buf_size(r, k, i)| & gets the size of a specific registered buffer. \\
  \midrule
  Zero-copy &  \\
  \midrule
  \spec|zerocopy_alloc(n)| & Allocate \spec|n| bytes of zero-copy memory. \\
  % shared between the guest application and the \worker.
  \spec|zerocopy_free(p)| & Free the zero-copy memory pointed to by \spec|p|. \\
%  \spec|zerocopy_get_physical_address(p)| & Get the physical address of the zero-copy buffer \spec|p|. For use with APIs which require physical addresses for DMA (e.g., Intel QuickAssist). \\
%  \midrule
%  Misc. &  \\
%  \midrule
%  \spec|strlen(s)| & return the length of a \spec|NULL| terminated string. \\
%  \spec|min(x, y)|/\spec|max(x, y)| & return the minimum/maximum of two values. \\
  \bottomrule
  \end{tabular}
  }
  \vspace{.1cm}
  \caption{An except from the \Lapis standard library for use in expressions and utility functions.
%  \amp{Remove \spec|_type| from \spec|declare_metadata_type(ty)|?}
%  \hyu{Vote for \spec|declare_metadata(type)|.}\amp{\spec|type| is a keyword in Lapis as presented. So we cannot use it there without confusion.}
}
  \label{tab:lapis-stdlib}
\end{table}

\paragraphbe{State Capture.} To support VM migration, \AvA must capture the state of API objects and recreate that state at the destination. This requires visibility into the relationships between API calls
and device state: \AvA cannot simply serialize, transfer, and deserialize device state to
effect a migration because \AvA's view of device state is through the high level API and through
semantics specified by the programmer.
\AvA splits object state into two categories based on descriptors from the \AvA programmer.
\emph{Explicit} state is serialized into \AvA-managed storage by programmer-defined functions;
\emph{implicit} state is captured by recording calls which mutate an \emph{object} (e.g. a buffer) based on \spec|obj_record| descriptors. In \autoref{fig:lapis-values-example}, \spec|obj_record| descriptor is used in
\spec|cuMemAlloc| to expose implicit state, in this case a device-side buffer which must be allocated to recreate state at the destination.

\paragraphbe{Runtime Library.}
The \Lapis standard library is illustrated (in excerpted form) in \autoref{tab:lapis-stdlib},
and is designed to support common features we encountered across multiple APIs.
State tracking often requires storing metadata about an API object, so the library provides a system to do that.
Many APIs support asynchronous functions for which buffers tracking is required, so the library provides a set of function to track buffers.
For APIs that require zero-copy data transfer for performance or because they expose hardware doorbells, the library provides memory management functions that expose \AvA's support for zero-copy buffers.
%In addition to those listed, t
The library also provides common utilities
%which are common in the expression which commute the size of buffers,
such as \spec|min|, \spec|max|, and \spec|strlen|.

%\cjr{This should describe common utility functions as well as whatever built-in eBPF policies we already provide.}
%\amp{I would really prefer to keep the eBPF stuff separate. It cannot be used along with the Lapis utilities and cannot even appear in the same file.}
%\cjr{OK, that's fine. We should still add a very brief description of what's in the table of library APIs. Doesn't need to be long.}


%\AvA allows for zero-copy data transfer for buffers that are allocated via API function (e.g., \spec|cuHostMemAlloc|).
%However, the current implementation only supports the limited case where the API allocation functions can be replaced with \AvA specific implementations using the zero-copy allocation functions listed in \autoref{tab:lapis-stdlib}.
%An improved implementation could use the original API functions and then make those buffers available in the guest application.


\input{ava/tables/generated-code-outline-guest}

\paragraphbe{Workflow.}
In the \AvA workflow, \CAvA is initially invoked on the API header files to generate a provisional \Lapis specification for all functions in the API;
the developer then refines that specification.
For perspective,
the actual provisional specification generated by \CAvA for the functions in Figure~\ref{fig:spec_example} totals 107~lines,
of which 29~lines are deleted, 9~lines are changed, and 21~lines are added to reach the final specification used in our evaluation.
%Utilities called in these functions total 17~lines.
%\hyu{total (cuLaunchKernel 51, cuMemcpyHtoDAsync 25, cuMemAlloc 17, cuStreamSynchronize 14),
%deleted (13,6,5,5), changed (4,3,2,0), and added (9,6,0,6).
%(\lstinline|cuLaunchKernel_extra_size| 6, \lstinline|__helper_load_async_buffer_list| 11.)}
\CAvA can infer the semantics of functions and arguments in simple cases and provides safe defaults when possible (e.g., by default, functions are synchronous and numeric types are opaque).
For cases that do not admit a conservative guess, \CAvA emits code that will cause an error, e.g., for ``direction'' (in/out parameters),
which provides programmer guidance about what additional information is required and where it should be expressed.

Writing a \Lapis specification requires user-level knowledge of the API.
The developer must understand the API function semantics but does not need to know how to implement the API or understand details of \AvA or API remoting.
Our experience is that most APIs (e.g., OpenCL~\cite{opencl}) provide documentation sufficient to achieve this.
However one API (the CUDA runtime API)
required \Lapis specifications for undocumented functions which required more developer effort to produce. The real target users of \AvA
are hypervisor developers, cloud providers, and accelerator vendor software engineers, for whom the required knowledge can be safely assumed, and for whom
the reduction in development effort is compelling even for complex APIs.

\subsection{Code Generation}
\label{s:code_gen}

% \CAvA generates C source code from the API specification using templating.
% \CAvA builds simplified intermediate representation of the specification, \CAvA-IR.
% \CAvA-IR is similar to \Lapis, but it applies a fixed set of descriptors to every value (e.g., argument) and combines all conditionals into expressions which compute specification values (e.g., the boolean representing that a function is synchronous).\amp{Is this clear?}\cjr{DISCUSS}

\CAvA generates several separate components for each API function, including a stub function, a call handler, a return handler, and a replay handler for VM migration.
%The stub functions and return handlers are in the guest library, while call handlers are in the \worker, except in the case of callbacks, for which the placement is reversed.
%Replay handlers are always in the \worker.

\input{ava/tables/generated-code-outline-worker}

The generated stubs, in \autoref{fig:generated-code-outline-guest}, construct a command from the arguments (including handling lists of asynchronous output buffers on lines~\ref{line:bufs-start}--\ref{line:bufs-end}) and then send it, via the hypervisor, to the \worker.
The hypervisor enforces resource sharing policy at the router based on the resource accounting and policy descriptors.
The \worker, in \autoref{fig:generated-code-outline-worker}, deserializes the arguments from the command and then performs the real call.
The results of the call are passed back to the guest lib in the same was as the call is made.
The \worker also transfers (lines~\ref{line:sync-bufs-start}--\ref{line:sync-bufs-end}) the shadow buffers registered during calls to \spec|cuMemcpyDtoHAsync| (line~\ref{line:register-buf}) back to the guest.
The guest lib handles the response by completing record-and-replay tracking, looking up the API call record, copying transferred shadow buffers into application buffers, and marking the call complete (code elided for brevity).

% The guest lib handles the response as shown in \autoref{fig:generated-code-outline-guest-handle}.
%The \Lapis specification provides all the information needed for \CAvA to generate serialize and deserialize the arguments since the specification provides enough information to traverse all the argument data structures.
%\amp{The example generated code here is for a function not in the example spec. We should update this code to match some function in the spec.}
%As an example Figures \ref{fig:generated-code-outline-structs}--\ref{fig:generated-code-outline-worker} show an outline of code generated for \spec|cuMemcpyHtoD| from \autoref{fig:lapis-values-example}.
%\input{ava/tables/generated-code-outline-guest-handle}

The \AvA API-agnostic components provide shadow buffer management primitives that the generated code uses to maintain \worker-side shadows of application buffers.
%Application buffer and shadow buffer lifetimes are coupled in the \worker is reused. ~\cjr{I think this is pretty much obvious to the point of implicit, and takes a lot of space to state precisely.}
\AvA's shadow buffers function as a caching layer that can buffer updates and apply them in batch.
In most cases, copy operations to synchronize shadow and application buffers are required only at API call boundaries,
so \AvA-controlled buffers are transparent to the guest, work without true shared memory between the guest and \worker, and are faster than page-granularity software shared memory.
%so \AvA-controlled buffers are indistinguishable from a buffer shared between the guest and the \worker, but work without shared memory and are faster than a page-level software shared memory system.
In cases where updates must be made visible in the guest without an API call to serve as a synchronization point,
true shared memory between the guest lib and the \worker can be specified using \Lapis's \spec|zerocopy| support.

Currently, \CAvA only supports C as an output language, but this is not a fundamental limitation of \AvA.
We expect implementations for C++ and Python to be straightforward.

\subsection{Mapped memory}
\label{s:api:mapped-mem}

\AvA does not currently map \worker host memory into guest application space by default.
However, \AvA still supports applications that use device-mapped memory by copying data between the guest and \worker.
The implementation uses \Lapis descriptors to track mapped buffers and ensure they are always
passed as contextual arguments to synchronization functions, e.g., \lstinline|cuSynchronizeStream|.
Importantly, the technique respects the semantics of the API: even without \AvA the only way an application can
\emph{guarantee} that device writes are visible to the application is to call a synchronization function.
However, some GPUs do make writes visible between synchronization functions and
research systems rely on it to implement accelerator-driven communication (e.g. GPUfs~\cite{gpufs}),
but will not function correctly with \AvA. The limitation is not fundamental, and we plan to address
it in future work.

\subsection{Resource accounting and scheduling}

\CAvA supports resource accounting using \Lapis descriptors which specify the type and quantity of resources each API function consumes.
To enforce resource sharing requirements, the code generator changes how API calls are handled by inserting accounting code in the router and hooks to call programmer-provided policy functions.
For continuous resources, the generated code may need to generate an artificial failure in response to an allocation request.
This requires that the compiler know how to fake a failure by constructing return values and/or executing specific code to change the library state.
For instantaneous resources, enforcement is implemented by delaying certain calls until other VMs have a chance to perform their instantaneous operations.

\CAvA generates code to compute resource usage information in the hypervisor from call arguments.
This code makes the call arguments available, similarly to \autoref{fig:generated-code-outline-worker} lines \ref{line:extract-start}--\ref{line:extract-end}.
Then executes the expression to compute the used resources (e.g., \spec|size| bytes of memory for \spec|cuMemAlloc|) and records the usage via an API-agnostic component of \AvA.

%\begin{figure}
%	\centering
%  \vspace{-1em}
%\begin{lstlisting}[language=c,style=nwspecc,numbers=left,escapechar=|,basicstyle={\footnotesize\ttfamily},belowskip=0em,aboveskip=0em]
%#include <CL/cl.h>|\label{line:include_cl}|
%throughput_rc commands_rc;|\label{line:declare_rc}|
%policy("policy_kern.c");|\label{line:scheduler_code}|
%cl_int clEnqueueTask(
%  cl_command_queue command_q, cl_kernel kernel,
%  cl_uint wait_list_len, cl_event *wait_list,
%  cl_event *event) {
%    if (event == NULL) async; else sync;|\label{line:sync_async}|
%    consumes_rc(commands_rc, 1);|\label{line:consume_rc}|
%    |$\cdots$| }
%\end{lstlisting}
%	\vspace*{-5pt}\caption{An example of \Lapis. The file \spec{policy_kern.c} is shown in Figure~\ref{fig:scheduler_example}.}
%	\label{fig:spec_example}
%\end{figure}

\begin{figure}
	\centering
\begin{lstlisting}[language=C,columns=flexible,mathescape,belowskip=0em,aboveskip=0em,basicstyle={\scriptsize\ttfamily},escapechar=|]
map_def cmd_cnt, priority;
kernel_calls = ava_get_rc_id("kernel_calls");
tot_id = 0; // ID of total in cmd_cnt
int consume(__sk_buff *skb) {
  vm_id = load_ava_vm_id(skb)
  amount = load_ava_rc_amount(skb, kernel_calls);
  vm_cnt = map_lookup_elem(&cmd_cnt, &vm_id);
  tot_cnt = map_lookup_elem(&cmd_cnt, &tot_id);
  fetch_and_add(vm_cnt, amount);|\label{line:bpf_count_begin}|
  fetch_and_add(tot_cnt, amount); }|\label{line:bpf_count_end}|
int schedule(__sk_buff *skb) {
  |\elidedcode{Load vm\_id, vm\_cnt, vm\_pri, tot\_cnt, and tot\_pri}|
  if((*vm_cnt) * (*tot_pri) <= (*vm_pri) * (*tot_cnt))|\label{line:bpf_prio_begin}|
    return HIGH_PRIO;
  else return LOW_PRIO; }|\label{line:bpf_prio_end}|
\end{lstlisting}
\vspace*{-5pt}\caption{An example eBPF policy program (simplified for clarity). This is referenced from Figure~\ref{fig:spec_example} as \spec{policy_kern.c}.
  % \amp{Would it be acceptable to remove the reference (\&) and dereference (*) operators?}\hyu{That's not a good idea. But we can simplify line 7-8,12 as ``load vm\_cnt, tot\_cnt from maps'', then we may remove the dereference from line 13.}
  % \amp{elide code to shorten.}\hyu{Shall we use smaller number for higher priority? It seems more practical for some policies.}\amp{I don't see any value in that. May as well keep it conceptually simple. A real implementation might flip the priorities, but that doesn't really matter.}
}
	\label{fig:scheduler_example}
\end{figure}

In Figure~\ref{fig:spec_example}, the specification of %\linebreak
\spec@cuLaunchKernel@ includes resource usage descriptors and a reference to a custom scheduler.
Figure~\ref{fig:scheduler_example} shows code for an eBPF based scheduler.
The \spec@consume@ function is called when the \worker reports resource utilization to the hypervisor, which
in this case, is every time an API call is made.
The \spec@schedule@ function is called whenever a command reaches the head of its VM's queue to compute the priority for the command.
The router dispatches commands in priority order (highest to lowest).
% \amp{This could result in many commands being sent to different workers which use the same device at the same time. We may need to limit the number of commands sitting in the \workers queue to some small value (2-3) to avoid \workers contending due to the execution time needed for the commands.}%
% \hyu{This is allowed as long as the device supports time sharing.}\amp{This would break policy enforcement because ALL outstanding commands could be dispatched at the same time.}
% \hyu{We can always schedule only one or some of the most prioritized commands. BTW that won't break the policy enforcement even if the hardware's time multiplexing is unfair if we only consider the ``end2end'' device execution time.}
% \amp{I think we are talking across each other. Let's talk after submission}
The \AvA policy functions take an \spec@__sk_buff@ argument containing the \AvA command information.
This allows \AvA to use the existing eBPF infrastructure directly.

The algorithm in Figure~\ref{fig:scheduler_example} is simplified for clarity.
It counts the number of commands each VM sends (\spec@consume@, lines~\ref{line:bpf_count_begin}--\ref{line:bpf_count_end}) and prioritizes VMs which have have sent less than their share of the total commands (\spec@schedule@, lines~\ref{line:bpf_prio_begin}--\ref{line:bpf_prio_end}).
A real policy would periodically reset or slowly reduce the counts and use additional information to properly handler varied command costs~\cite{sched_survey,rossbach2011ptask,aimd}.

% \reviewer{A}{Annotating the API seems to require significant knowledge about the hardware (e.g., specifying the type and quantity of resources each call consumes, specifying which objects are modified). These descriptors also seem to assume that the side effect of API calls are uniform across different hardware versions, etc.}


\endgroup
% \vspace{-1.5em}

